import streamlit as st
import os

from langchain.memory import ConversationBufferMemory
from utils import qa_agent
from dotenv import load_dotenv

load_dotenv()

index_name = os.environ.get("INDEX_NAME")
if not index_name:
    raise ValueError("ç’°å¢ƒè®Šæ•¸ 'INDEX_NAME' æœªè¨­å®š")

print(index_name)

st.title("ğŸ“‘ æ™ºæ…§PDF")

with st.sidebar:
    openai_api_key = st.text_input("è«‹è¼¸å…¥OpenAI APIé‡‘é‘°ï¼š", type="password")
    st.markdown("[OpenAI API key èªªæ˜](https://platform.openai.com/account/api-keys)")

if "memory" not in st.session_state:
    st.session_state["memory"] = ConversationBufferMemory(
        return_messages=True,
        memory_key="chat_history",
        output_key="answer"
    )

uploaded_file = st.file_uploader("ä¸Šå‚³ä½ çš„PDFæ–‡ä»¶ï¼š", type="pdf")
question = st.text_input("å°PDFå…§å®¹é€²è¡Œæå•", disabled=not uploaded_file)

if uploaded_file and question and not openai_api_key:
    st.info("è¼¸å…¥API key é‡‘é‘°")

if uploaded_file and question and openai_api_key:
    with st.spinner("AIæ€è€ƒä¸­ï¼Œè«‹ç¨å€™..."):
        response = qa_agent(openai_api_key, st.session_state["memory"],
                            index_name,
                            uploaded_file, question)
    st.write("### ç­”æ¡ˆ")
    st.write(response["answer"])
    st.session_state["chat_history"] = response["chat_history"]

if "chat_history" in st.session_state:
    with st.expander("æ­·å²è¨Šæ¯"):
        for i in range(0, len(st.session_state["chat_history"]), 2):
            human_message = st.session_state["chat_history"][i]
            ai_message = st.session_state["chat_history"][i + 1]
            st.write(human_message.content)
            st.write(ai_message.content)
            if i < len(st.session_state["chat_history"]) - 2:
                st.divider()
